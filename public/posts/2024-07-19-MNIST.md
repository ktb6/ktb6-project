---
title: 'MNIST NN layer 수에 따른 비교'
date: '2024-07-19'
author: 'kevin.lee'
description: 'MNIST NN'

category: 'Blog'
image: ''
tags: ['NN']
---


3-Layer NN 구현하기

```python
class ThreeLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, 50)
        self.params['b2'] = np.zeros(50)
        self.params['W3'] = weight_init_std * np.random.randn(50, output_size)
        self.params['b3'] = np.zeros(output_size)

    def predict(self, x):
        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']
        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        z2 = sigmoid(a2)
        a3 = np.dot(z2, W3) + b3

        y = softmax(a3)

        return y

    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy_error(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])
        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])

        return grads

    def gradient(self, x, t):
        W1, W2, W3 = self.params['W1'], self.params['W2'], self.params['W3']
        b1, b2, b3 = self.params['b1'], self.params['b2'], self.params['b3']
        grads = {}

        batch_num = x.shape[0]

        # forward
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)

        a2 = np.dot(z1, W2) + b2
        z2 = sigmoid(a2)

        a3 = np.dot(z2, W3) + b3
        y = softmax(a3)

        # backward
        dy = (y - t) / batch_num
        grads['W3'] = np.dot(z2.T, dy)
        grads['b3'] = np.sum(dy, axis=0)

        da2 = np.dot(dy, W3.T)
        dz2 = sigmoid_grad(a2) * da2

        grads['W2'] = np.dot(z1.T, dz2)
        grads['b2'] = np.sum(dz2, axis=0)

        da1 = np.dot(da2, W2.T)
        dz1 = sigmoid_grad(a1) * da1

        grads['W1'] = np.dot(x.T, dz1)
        grads['b1'] = np.sum(dz1, axis=0)

        return grads
```

<pre>
train acc, test acc, loss | 0.0993, 0.1032, 2.2973440828292224
train acc, test acc, loss | 0.11236666666666667, 0.1135, 2.2976294527262553
train acc, test acc, loss | 0.10441666666666667, 0.1028, 2.2981164820326243
train acc, test acc, loss | 0.11236666666666667, 0.1135, 2.294309083545785
train acc, test acc, loss | 0.29481666666666667, 0.2968, 1.8331147080033532
train acc, test acc, loss | 0.5577333333333333, 0.5517, 1.270274783542825
train acc, test acc, loss | 0.7617666666666667, 0.7681, 0.8147502567511475
train acc, test acc, loss | 0.8453333333333334, 0.8524, 0.47400378411384764
train acc, test acc, loss | 0.8793, 0.885, 0.34291781812862665
train acc, test acc, loss | 0.8993333333333333, 0.9024, 0.37357185657902503
train acc, test acc, loss | 0.9140166666666667, 0.9153, 0.3107757743795986
train acc, test acc, loss | 0.9240833333333334, 0.9242, 0.20247812119947511
train acc, test acc, loss | 0.9336166666666667, 0.9329, 0.27441546895569296
train acc, test acc, loss | 0.9383, 0.9365, 0.24584240203258376
train acc, test acc, loss | 0.9441333333333334, 0.9427, 0.23153601658251283
train acc, test acc, loss | 0.9482, 0.9474, 0.25121415498975397
train acc, test acc, loss | 0.9515, 0.9509, 0.15593526271388572
</pre>
![loss](/images/kevin/3layer_acc.png)
![loss](/images/kevin/3layer_loss.png)



1-Layer NN 구현하기

```python
class OneLayerNet:

    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 가중치 초기화
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, output_size)
        self.params['b1'] = np.zeros(output_size)

    def predict(self, x):
        W1 = self.params['W1']
        b1 = self.params['b1']

        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)

        y = softmax(a1)

        return y

    # x : 입력 데이터, t : 정답 레이블
    def loss(self, x, t):
        y = self.predict(x)

        return cross_entropy_error(y, t)

    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)

        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy

    # x : 입력 데이터, t : 정답 레이블
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)

        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])

        return grads

    def gradient(self, x, t):
        W1= self.params['W1']
        b1 = self.params['b1']
        grads = {}

        batch_num = x.shape[0]

        # forward
        a1 = np.dot(x, W1) + b1
        y = sigmoid(a1)

        # backward
        dy = (y - t) / batch_num
        grads['W1'] = np.dot(x.T, dy)
        grads['b1'] = np.sum(dy, axis=0)


        return grads
```
<pre>
train acc, test acc, loss | 0.2116, 0.2067, 2.088272754207128
train acc, test acc, loss | 0.88585, 0.8951, 0.2805937349496835
train acc, test acc, loss | 0.8958666666666667, 0.9023, 0.3992988592077825
train acc, test acc, loss | 0.9002, 0.907, 0.15175853545318108
train acc, test acc, loss | 0.90415, 0.9105, 0.41369044227032886
train acc, test acc, loss | 0.9061, 0.9121, 0.2996602430532773
train acc, test acc, loss | 0.9077166666666666, 0.9141, 0.3130279475719507
train acc, test acc, loss | 0.9087333333333333, 0.9121, 0.1687840082003685
train acc, test acc, loss | 0.9099166666666667, 0.9151, 0.40659936606900365
train acc, test acc, loss | 0.9097833333333334, 0.914, 0.2556797556225938
train acc, test acc, loss | 0.9113166666666667, 0.9142, 0.20387065498976972
train acc, test acc, loss | 0.9126833333333333, 0.9155, 0.29361594612780684
train acc, test acc, loss | 0.9129, 0.9182, 0.3133149367274704
train acc, test acc, loss | 0.9134333333333333, 0.9186, 0.33830923161195686
train acc, test acc, loss | 0.9139166666666667, 0.917, 0.12435216767520174
train acc, test acc, loss | 0.91415, 0.9175, 0.27991690708773326
train acc, test acc, loss | 0.9156333333333333, 0.9189, 0.3164468470662784
</pre>
![loss](/images/kevin/1layer_acc.png)
![loss](/images/kevin/1layer_loss.png)


1-layer
train acc   :   0.9156333333333333
test acc    :   0.9189
loss        :   0.3164468470662784

2-layer
train acc   :   0.9459833333333333
test acc    :   0.9421
loss        :   0.2243502736741579

3-layer
train acc   :   0.9515
test acc    :   0.9509
loss        :   0.15593526271388572

